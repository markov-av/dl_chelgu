{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 24985.033995\n",
      "Epoch 1, loss: 26486.274974\n",
      "Epoch 2, loss: 25922.768044\n",
      "Epoch 3, loss: 27557.151243\n",
      "Epoch 4, loss: 26840.451264\n",
      "Epoch 5, loss: 27066.749694\n",
      "Epoch 6, loss: 26616.494448\n",
      "Epoch 7, loss: 25624.808225\n",
      "Epoch 8, loss: 25489.122039\n",
      "Epoch 9, loss: 27247.364592\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7012aaf310>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.173\n",
      "Epoch 0, loss: 20324.493777\n",
      "Epoch 1, loss: 20172.485353\n",
      "Epoch 2, loss: 20112.867929\n",
      "Epoch 3, loss: 20073.906524\n",
      "Epoch 4, loss: 20042.209017\n",
      "Epoch 5, loss: 20016.843828\n",
      "Epoch 6, loss: 19996.117472\n",
      "Epoch 7, loss: 19980.977424\n",
      "Epoch 8, loss: 19976.701602\n",
      "Epoch 9, loss: 19965.434325\n",
      "Epoch 10, loss: 19961.571849\n",
      "Epoch 11, loss: 19957.436472\n",
      "Epoch 12, loss: 19949.544685\n",
      "Epoch 13, loss: 19954.489396\n",
      "Epoch 14, loss: 19950.621553\n",
      "Epoch 15, loss: 19945.151788\n",
      "Epoch 16, loss: 19948.172731\n",
      "Epoch 17, loss: 19943.380522\n",
      "Epoch 18, loss: 19947.852836\n",
      "Epoch 19, loss: 19940.421488\n",
      "Epoch 20, loss: 19949.569788\n",
      "Epoch 21, loss: 19943.663327\n",
      "Epoch 22, loss: 19940.273747\n",
      "Epoch 23, loss: 19940.001586\n",
      "Epoch 24, loss: 19942.053498\n",
      "Epoch 25, loss: 19941.294751\n",
      "Epoch 26, loss: 19944.089822\n",
      "Epoch 27, loss: 19941.686876\n",
      "Epoch 28, loss: 19949.965544\n",
      "Epoch 29, loss: 19944.122716\n",
      "Epoch 30, loss: 19944.123204\n",
      "Epoch 31, loss: 19942.358568\n",
      "Epoch 32, loss: 19946.433051\n",
      "Epoch 33, loss: 19940.882026\n",
      "Epoch 34, loss: 19938.888814\n",
      "Epoch 35, loss: 19938.985332\n",
      "Epoch 36, loss: 19947.530319\n",
      "Epoch 37, loss: 19941.958502\n",
      "Epoch 38, loss: 19938.989286\n",
      "Epoch 39, loss: 19940.875200\n",
      "Epoch 40, loss: 19945.831333\n",
      "Epoch 41, loss: 19945.933229\n",
      "Epoch 42, loss: 19939.229813\n",
      "Epoch 43, loss: 19942.170254\n",
      "Epoch 44, loss: 19941.224461\n",
      "Epoch 45, loss: 19942.520065\n",
      "Epoch 46, loss: 19941.323799\n",
      "Epoch 47, loss: 19944.178459\n",
      "Epoch 48, loss: 19944.713743\n",
      "Epoch 49, loss: 19939.637820\n",
      "Epoch 50, loss: 19943.930102\n",
      "Epoch 51, loss: 19939.129028\n",
      "Epoch 52, loss: 19938.033998\n",
      "Epoch 53, loss: 19948.351020\n",
      "Epoch 54, loss: 19938.113156\n",
      "Epoch 55, loss: 19939.448058\n",
      "Epoch 56, loss: 19941.600254\n",
      "Epoch 57, loss: 19942.703426\n",
      "Epoch 58, loss: 19947.907044\n",
      "Epoch 59, loss: 19939.535619\n",
      "Epoch 60, loss: 19950.547475\n",
      "Epoch 61, loss: 19946.686344\n",
      "Epoch 62, loss: 19942.812976\n",
      "Epoch 63, loss: 19941.093758\n",
      "Epoch 64, loss: 19941.583273\n",
      "Epoch 65, loss: 19947.729076\n",
      "Epoch 66, loss: 19943.014092\n",
      "Epoch 67, loss: 19947.268994\n",
      "Epoch 68, loss: 19935.434508\n",
      "Epoch 69, loss: 19947.698694\n",
      "Epoch 70, loss: 19938.995831\n",
      "Epoch 71, loss: 19945.345736\n",
      "Epoch 72, loss: 19944.629205\n",
      "Epoch 73, loss: 19936.167100\n",
      "Epoch 74, loss: 19946.065744\n",
      "Epoch 75, loss: 19938.359796\n",
      "Epoch 76, loss: 19943.084719\n",
      "Epoch 77, loss: 19942.057630\n",
      "Epoch 78, loss: 19945.580615\n",
      "Epoch 79, loss: 19941.893093\n",
      "Epoch 80, loss: 19943.776691\n",
      "Epoch 81, loss: 19933.827496\n",
      "Epoch 82, loss: 19945.520197\n",
      "Epoch 83, loss: 19945.620218\n",
      "Epoch 84, loss: 19938.932387\n",
      "Epoch 85, loss: 19944.581255\n",
      "Epoch 86, loss: 19947.577613\n",
      "Epoch 87, loss: 19935.112778\n",
      "Epoch 88, loss: 19945.686089\n",
      "Epoch 89, loss: 19940.358841\n",
      "Epoch 90, loss: 19944.371753\n",
      "Epoch 91, loss: 19945.311577\n",
      "Epoch 92, loss: 19943.066215\n",
      "Epoch 93, loss: 19946.431663\n",
      "Epoch 94, loss: 19940.672148\n",
      "Epoch 95, loss: 19943.625149\n",
      "Epoch 96, loss: 19943.055460\n",
      "Epoch 97, loss: 19942.656520\n",
      "Epoch 98, loss: 19945.530512\n",
      "Epoch 99, loss: 19948.329043\n",
      "Accuracy after training for 100 epochs:  0.231\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-4, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
